/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */
package eu.opensme.cope.factgenerators.lsa;

/**
 *
 * @author econst
 */
import edu.ucla.sspace.text.CompoundWordIterator;
import edu.ucla.sspace.text.FilteredIterator;
import edu.ucla.sspace.text.LimitedWordIterator;
import edu.ucla.sspace.text.OrderPreservingFilteredIterator;
import edu.ucla.sspace.text.TokenFilter;
import edu.ucla.sspace.text.WordIterator;
import edu.ucla.sspace.text.WordReplacementIterator;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOError;
import java.io.IOException;
import java.io.StringReader;

import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedHashSet;
import java.util.Map;
import java.util.Properties;
import java.util.Set;


/**
 * A factory class for generating {@code Iterator<String>} tokenizers for
 * streams of tokens such as {@link BufferedReader} instances.  This class
 * manages all of the internal configurations and properties for how to
 * tokenize.  {@link edu.ucla.sspace.common.SemanticSpace SemanticSpace}
 * instances are encouraged to utilize this class for creating iterators over
 * the tokens in the documents rather than creating the iterators themsevles, as
 * this class may contain additional settings to be applied to which the {@link
 * edu.ucla.sspace.common.SemanticSpace SemanticSpace} instance would not have
 * access.
 *
 * <p>
 *
 * This class offers two configurable parameters for controlling the tokenizing
 * of streams.
 *
 * <dl style="margin-left: 1em">
 *
 * <dt> <i>Property:</i> <code><b>{@value #TOKEN_FILTER_PROPERTY}
 *      </b></code> <br>
 *      <i>Default:</i> {@code null}
 *
 * <dd style="padding-top: .5em">This property sets a configuration of a {@link
 *      TokenFilter} that should be applied to all token streams.<p>
 *
 *
 * <dt> <i>Property:</i> <code><b>{@value #COMPOUND_TOKENS_FILE_PROPERTY}
 *      </b></code> <br>
 *      <i>Default:</i> {@code null}
 *
 * <dd style="padding-top: .5em">This property sets the name of a file that
 *      contains all of the recognized compound words (or multi-token tokens)
 *      recognized by any iterators returned by this class.<p>
 *
 * </dl> <p>
 *
 * <p> 
 *
 * Note that tokens will be combined into a compound token prior to filtering.
 * Therefore if filtering is enabled, any compound token should also be
 * permitted by the word filter.<p>
 *
 * Note that this class provides two distinct ways to access the token streams
 * if filtering is enabled.  The {@link #tokenize(BufferedReader) tokenize}
 * method will filter out any tokens without any indication.  This can
 * significantly alter the original ordering of the token stream.  For
 * applications where the original ordering needs to be preserved, the {@link
 * #tokenizeOrdered(BufferedReader) tokenizeOrdered} method should be used
 * instead.  This method will return the {@code IteratorFactor.EMTPY_TOKEN}
 * value to indicate that a token has been removed.  This preserves the original
 * token ordering without requiring applications to do the filtering themselves.
 * Note that If filtering is disabled, the two methods will return the same
 * tokens.<p>
 *
 * This class is thread-safe.
 *
 * @see WordIterator
 * @see TokenFilter
 * @see CompoundWordIterator
 */
public class IterFactory {

    /**
     * The signifier that stands in place of a token has been removed from an
     * iterator's token stream by means of a {@link TokenFilter}.  Tokens
     * returned by {@link #tokenizeOrdered(BufferedReader) tokenizeOrdered} may
     * be checked against this value to determine whether a token at that
     * position in the stream would have been returned but was removed.
     */
    public static final String EMPTY_TOKEN = "";

    /** 
     * The prefix for naming publically accessible properties
     */
    private static final String PROPERTY_PREFIX =
        "edu.ucla.sspace.text.TokenizerFactory";

    /**
     * Specifies the {@link TokenFilter} to apply to all iterators generated by
     * this factory
     */
    public static final String TOKEN_FILTER_PROPERTY = 
        PROPERTY_PREFIX + ".tokenFilter";

    /**
     * Specifies whether or not steming should be done on tokens.
     */
    public static final String USE_STEMMING_PROPERTY =
        PROPERTY_PREFIX + ".stem";

    /**
     * Specifies the name of a file that contains all the recognized compound
     * tokens
     */
    public static final String COMPOUND_TOKENS_FILE_PROPERTY = 
        PROPERTY_PREFIX + ".compoundTokens";
    
    /**
     * Specifies the name of a file which contains term replacement mappings for
     * a {@code WordReplacementIterator}.
     */
    public static final String TOKEN_REPLACEMENT_FILE_PROPERTY =
        PROPERTY_PREFIX + ".replacementTokens";

    /**
     * Specifices an upper limit on the number of tokens each iterator can
     * return.
     */
    public static final String TOKEN_COUNT_LIMIT_PROPERTY =
        PROPERTY_PREFIX + ".tokenCountLimit";

    /**
     * An optional {@code TokenFilter} to use to remove tokens from document
     */
    private static TokenFilter filter;
    
    /**
     * True if stemming should be done in a word iterator.
     */
    private static boolean useStemming;

    /**
     * The maximum number of tokens an iterator may return.
     */
    private static int wordLimit;

    /**
     * An optional {@code Map} used to replace terms returned by iterators.
     */
    private static Map<String, String> replacementMap;

    /**
     * A mapping from a thread that is currently processing tokens to the {@link
     * CompoundWordIterator} doing the tokenizing if compound word support is
     * enabled.  This mapping is required for two reasons.  One to reduce the
     * overhead of creating {@code CompoundWordIterators} by calling {@code
     * reset} on them; and two, to provide a way for any updates to the list of
     * compound words to propagate to the threads that process them.
     */
    private static final Map<Thread,CompoundWordIterator> compoundIterators =
        new HashMap<Thread,CompoundWordIterator>();

    /**
     * The set of compound tokens recognized by the system or {@code null} if
     * none are recognized
     */
    private static Set<String> compoundTokens = null;

    /**
     * Uninstantiable
     */
    private IterFactory() { }

    /**
     * Reconfigures the type of iterator returned by this factory based on the
     * specified properties.
     */
    public static synchronized void setProperties(Properties props) {
        wordLimit = Integer.parseInt(
                props.getProperty(TOKEN_COUNT_LIMIT_PROPERTY, "0"));

        String filterProp = 
            props.getProperty(TOKEN_FILTER_PROPERTY);
        filter = (filterProp != null)
            ? TokenFilter.loadFromSpecification(filterProp)
            : null;

        useStemming = props.getProperty(USE_STEMMING_PROPERTY) != null;

        String compoundTokensProp = 
            props.getProperty(COMPOUND_TOKENS_FILE_PROPERTY);
        if (compoundTokensProp != null) {
            File compoundTokensFile = new File(compoundTokensProp);
            if (!compoundTokensFile.exists()) {
                throw new IllegalArgumentException(COMPOUND_TOKENS_FILE_PROPERTY
                    + " is set to a non-existant file: " + compoundTokensProp);
            }
            
            // Load the tokens from file
            compoundTokens = new LinkedHashSet<String>();
            try {
                BufferedReader br = 
                    new BufferedReader(new FileReader(compoundTokensFile));
                for (String line = null; (line = br.readLine()) != null; ) {
                    compoundTokens.add(line);
                }
                // For any currently processing threads, update their mapped
                // iterator with the new set of tokens
                for (Map.Entry<Thread,CompoundWordIterator> e
                     : compoundIterators.entrySet()) {
                    // Create an empy dummy BufferedReader, which will be
                    // discarded upon the next .reset() call to the iterator
                    BufferedReader dummyBuffer =
                        new BufferedReader(new StringReader(""));
                    e.setValue(new CompoundWordIterator(
                                dummyBuffer, compoundTokens));
                }
            } catch (IOException ioe) {
                // rethrow
                throw new IOError(ioe);
            }
        } else {
            // If the user did not specify a set of compound tokens, null out
            // the set, in the event that there was one previously
            compoundTokens = null;
        }

        String replacementProp = 
            props.getProperty(TOKEN_REPLACEMENT_FILE_PROPERTY);
        if (replacementProp != null) {
            try {
                BufferedReader br =
                    new BufferedReader(new FileReader(replacementProp));
                replacementMap = new HashMap<String, String>();
                String line = null;
                while ((line = br.readLine()) != null) {
                    String[] termReplacement = line.split("\\s+");
                    replacementMap.put(termReplacement[0], termReplacement[1]);
                }
            } catch (IOException ioe) {
                throw new IOError(ioe);
            }
        } else
            replacementMap = null;
    }

    /**
     * Tokenizes the contents of the reader according to the system
     * configuration and returns an iterator over all the tokens, excluding
     * those that were removed by any configured {@link TokenFilter}.
     *
     * @param reader a reader whose contents are to be tokenized
     *
     * @return an iterator over all of the optionally-filtered tokens in the
     *         reader
     */
    public static Iterator<String> tokenize(BufferedReader reader) {
        Iterator<String> baseIterator = getBaseIterator(reader);

        // If a filter is enabled, wrap the base tokenizer
        return (filter == null) 
            ? baseIterator : new FilteredIterator(baseIterator, filter);    
    }

    /**
     * Tokenizes the contents of the string according to the system
     * configuration and returns an iterator over all the tokens, excluding
     * those that were removed by any configured {@link TokenFilter}.
     *
     * @param str a string whose contents are to be tokenized
     *
     * @return an iterator over all of the optionally-filtered tokens in the
     *         string
     */
    public static Iterator<String> tokenize(String str) {
        return tokenize(new BufferedReader(new StringReader(str)));
    }
    
    /**
     * Tokenizes the contents of the reader according to the system
     * configuration and returns an iterator over all the tokens where any
     * removed tokens have been replaced with the {@code
     * IteratorFactory.EMPTY_TOKEN} value.  Tokens returned by this method may
     * be checked against this value to determine whether a token at that
     * position in the stream would have been returned but was removed.  In
     * doing this, the original order and positioning is retained.
     *
     * @param reader a reader whose contents are to be tokenized
     *
     * @return an iterator over all of the tokens in the reader where any tokens
     *         removed due to filtering have been replaced with the {@code
     *         IteratorFactory.EMPTY_TOKEN} value
     */
    public static Iterator<String> tokenizeOrdered(BufferedReader reader) {
        Iterator<String> baseIterator = getBaseIterator(reader);

        // If a filter is enabled, wrap the base tokenizer
        return (filter == null) 
            ? baseIterator
            : new OrderPreservingFilteredIterator(baseIterator, filter);
    }

    /**
     * Tokenizes the contents of the string according to the system
     * configuration and returns an iterator over all the tokens where any
     * removed tokens have been replaced with the {@code
     * IteratorFactory.EMPTY_TOKEN} value.  Tokens returned by this method may
     * be checked against this value to determine whether a token at that
     * position in the stream would have been returned but was removed.  In
     * doing this, the original order and positioning is retained.
     *
     * @param str a string whose contents are to be tokenized
     *
     * @return an iterator over all of the tokens in the string where any tokens
     *         removed due to filtering have been replaced with the {@code
     *         IteratorFactory.EMPTY_TOKEN} value
     */
    public static Iterator<String> tokenizeOrdered(String str) {
        return tokenizeOrdered(new BufferedReader(new StringReader(str)));
    }

    /**
     * Wraps an iterator returned by {@link tokenizeOrdered} to also include
     * term replacement of tokens.  Terms will be replaced based on a mapping
     * provided through the system configuration.
     *
     * @param reader A reader whose contents are to be tokenized.
     *
     * @return An iterator over all the tokens in the reader where any tokens 
     *         removed due to filtering have been replaced with the {@code
     *         IteratorFactory.EMPTY_TOKEN} value, and tokens may be replaced
     *         based on system configuration.
     */
    public static Iterator<String> tokenizeOrderedWithReplacement(
            BufferedReader reader) {
        Iterator<String> baseIterator = tokenizeOrdered(reader);
        return (replacementMap == null)
            ? baseIterator
            : new WordReplacementIterator(baseIterator, replacementMap);
    }

    /**
     * Returns an iterator for the basic tokenization of the stream before
     * filtering has been applied to the tokens.
     *
     * @param reader a reader whose contents are to be tokenized
     *
     * @return an iterator over the tokens in the stream
     */
    private static Iterator<String> getBaseIterator(BufferedReader reader) {
        // The base iterator is how the stream will be tokenized prior to any
        // filtering
        Iterator<String> baseIterator = null;

        // If a Set of compound tokens has been set, then create the underlying
        // iterator to recognize those tokens
        if (compoundTokens != null) {
            // Because the initialization step for a CWI has some overhead, use
            // the reset to keep the same tokens.  However, multiple threads may
            // be each using their own CWI, so keep Thread-local storage of what
            // CWI is being used to avoid resetting another thread's iterator.
            CompoundWordIterator cwi = 
            compoundIterators.get(Thread.currentThread());
            if (cwi == null) {
                cwi = new CompoundWordIterator(reader, compoundTokens);
                compoundIterators.put(Thread.currentThread(), cwi);
            } else {
                // NOTE: if the underlying set of valid compound words is ever
                // changed, the iterator returned from the compoundIterators map
                // will have been updated by the setProperties() call, so this
                // method is guaranteed to pick up the latest set of compound
                // words
                cwi.reset(reader);
            }
            baseIterator = cwi;
        } else {
            // Otherwise, just return a standard iterator over all the tokens
            // with no compounding
            baseIterator = new WordIter(reader, useStemming);
        }

        // If the word limit is less than 1, use the standard iterator.
        // Otherwise use a LimitedWordIterator.
        return (wordLimit < 1)
            ? baseIterator
            : new LimitedWordIterator(baseIterator, wordLimit);
    }

}
